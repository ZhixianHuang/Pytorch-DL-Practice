import torch

x_data = [1.0, 2.0, 3.0]
y_data = [2.0, 4.0, 6.0]

w = torch.tensor([1.0])  # w权重 是一个Tensor 因为W * X是张量运算              H(1) =   W  * X
                         # 例如一个5参数输入X(5*1) 输出是第一隐藏层H(1) (6*1)  [6*1] = [6*5][5*1]
w.requires_grad = True   # 计算图中需要保留梯度
                         # 一般的计算图在计算张量的时候不会自动计算他的梯度
                         # 梯度的获取 下方的w.grad获取梯度/w.grad.item()提取单个 ∂Loss/∂w 的浮点数(适合只有一个数值)
                         # 此时w是一个同时包含数值和梯度的张量 数值部分为w.data,梯度部分为w.grad
                         
'''
在PyTorch中，一个张量可以同时包含数值和梯度信息。你可以分别获取它们：
使用 w.data 可以获取张量 w 的数值部分，这是一个不带梯度信息的张量。
使用 w.grad 可以获取张量 w 的梯度信息，这是一个与 w 形状相同的张量，包含了相应位置的梯度值。
'''

def forward(x):
    return x * w  # 由于w是Tensor *运算符会自动进行重载 x也会被自动的类型转换转换为Tensor 以便能够处理张量之间的乘法
                  # 此类运算会生成一个运算图  当执行类似乘法、加法等运算时，深度学习框架会在内部构建一个计算图，用于表示整个计算过程。这个图形包含了操作和操作之间的依赖关系。
                  # 重载运算符指的是一个运算符有多个功能，但是作用的对象不相同
                  # w的权重需要被计算，所以前馈的计算的权重梯度也会被记住

def loss(x, y):
    y_pred = forward(x)  # 这里的损失函数loss计算 由于没有使用.data 也是一个张量运算 所以也会自动生成一个运算图
    return (y_pred - y) ** 2

# 所以每次调用一次forward()函数和loss()函数都会动态构建一个运算图
# 为什么要构建运算图？
# 方便自动求导，方便提取中间节点的结果，结构化体现计算过程，提供可视化模型结构，动态计算


print("Predict (before training)", 4, forward(4).item())

for epoch in range(100):
    for x_val, y_val in zip(x_data, y_data):
        l = loss(x_val, y_val)  # 前馈计算loss
        l.backward()  # 自动把刚才画的计算图上的这条计算链上的所有要求保存梯度的地方，求出并保存（需要保存w的梯度，所以所有需要w的地方的梯度都会被保存，包括w的链上所有的后续节点）
                      # 在深度学习中，通常有两种情况会导致梯度被保存：
                      # 需要更新的参数：这是最常见的情况。如果一个节点是一个需要更新的模型参数（例如权重），那么与该参数相关的梯度会在反向传播时被计算和保存。
                      # 需要反向传播的节点：如果一个节点在计算图中被使用到，并且你希望计算它的梯度，那么它的梯度也会被计算和保存。
                      # 运行完backward() 计算图就会被释放，因为存在计算图可能不同的神经网络（Dropout？）所以释放进行下次生成运算图，动态生成运算图
        print('\tgrad=', x_val, y_val, w.grad.item())  # 提取梯度的数值 得到标量 .item()适用于只有一个元素的张量
                                                       # 如果想获取整个张量的数值部分，可以使用 w.data 或 w.detach()，它们都会返回一个不带梯度信息的张量。
                                                       # 如果需要分别获取数值和梯度，可以直接使用 w.data 或者 w.detach()，然后再分别处理数值和梯度部分
                                                       # 此处的w.grad.item()是当前的x_val,y_val得到的 ∂Loss/∂w 提取为单个浮点数
        '''
        w.grad 获取张量 w 的梯度信息，它返回一个与 w 形状相同的张量，包含了相应位置的梯度值。
        w.grad.item() 将梯度值提取为一个标量（Python 数值），适用于只有一个元素的张量。
        '''
        
        # w.data = w.data - 0.01 * w.grad.item()
        w.data = w.data - 0.01 * w.grad.data  # 如果w = w - 0.01 * w.grad直接进行运算 其实是生成一个运算图
                                              # 我们只是想对于梯度进行数值计算 所以只需要进行数值运算操作就可以
                                              # 所以 w新数值= w老数值 - 学习率 * w梯度数值
        w.grad.data.zero_()                   # 清空梯度数值 因为在部分计算损失的时候会需要损失累加 ∂Loss1/∂w + ∂Loss2/∂w
                                              # 对于目前的情况 需要清空Loss
        
    print('process', epoch, l.item())

print("Predict (after training)", 4, forward(4).item())